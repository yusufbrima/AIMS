{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "id": "ggW4zZt5_uy-"
   },
   "outputs": [],
   "source": [
    "#@title Evironment Setup and imports\n",
    "#%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# from celluloid import Camera  # getting the camera\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.applications import VGG16, ResNet50, VGG19, DenseNet121, InceptionV3, Xception\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization, Activation\n",
    "from keras.constraints import maxnorm\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras_preprocessing.image.dataframe_iterator import DataFrameIterator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
    "from matplotlib.offsetbox import (OffsetImage,\n",
    "                                  AnnotationBbox)\n",
    "from IPython.display import HTML  # to show the animation in Jupyter\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from google.colab import files\n",
    "import cv2\n",
    "import glob\n",
    "from PIL import Image\n",
    "#we are importing custom classes from project files\n",
    "from eda import EDA\n",
    "from preprocessing import Preprocessing\n",
    "from modelling import Modelling\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": "OK"
      }
     }
    },
    "id": "QBMmDMVtAAbg",
    "outputId": "7e5a8e67-accd-42fc-deee-f4763b74bbd3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-e7ca8a3c-0609-4f08-a0b1-b2cbc18cb16c\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-e7ca8a3c-0609-4f08-a0b1-b2cbc18cb16c\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
      "covid19-radiography-database.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    }
   ],
   "source": [
    "#@title  We download the dataset from kaggle\n",
    "files.upload()\n",
    "!mkdir ~/.kaggle \n",
    "! cp kaggle.json ~/.kaggle/\n",
    "! chmod 600 ~/.kaggle/kaggle.json\n",
    "!kaggle datasets download -d tawsifurrahman/covid19-radiography-database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3FT1dySbBNSi"
   },
   "outputs": [],
   "source": [
    "! unzip covid19-radiography-database.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7bV6ytmoBYY0"
   },
   "outputs": [],
   "source": [
    "base = '/content/COVID-19_Radiography_Dataset'\n",
    "paths   = os.listdir(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "form",
    "id": "lUpWvAlwHx7L"
   },
   "outputs": [],
   "source": [
    "#@title Exploratory Data Analysis Class\n",
    "class EDA:\n",
    "    data = None\n",
    "    plots = {}\n",
    "    root = None\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "    def load_data(self):\n",
    "        # We are creating our test set\n",
    "        paths = os.listdir(self.root)\n",
    "        base = self.root\n",
    "        ls = {'path': [], 'class': [], 'mean': [], 'std': [], 'min': [], 'max': [], 'image': []}\n",
    "        if os.path.isdir(self.root) is True:\n",
    "            paths = os.listdir(self.root)\n",
    "            pr = 10.\n",
    "            for k in range(len(paths)):\n",
    "                if os.path.isdir(os.path.join(base, paths[k])):\n",
    "                    for f in os.listdir(os.path.join(base, paths[k])):\n",
    "                        file = os.path.join(os.path.join(base, paths[k]), f)\n",
    "                        arr_img = cv2.imread(file)\n",
    "                        mean = arr_img.mean()\n",
    "                        std = arr_img.std()\n",
    "                        minimum = arr_img.min()\n",
    "                        maximum = arr_img.max()\n",
    "                        clas = f.split('/')[-1].split('.')\n",
    "                        classes = clas[0].split('-')[0]\n",
    "                        ls['path'].append(file)\n",
    "                        ls['class'].append(classes)\n",
    "                        ls['mean'].append(mean)\n",
    "                        ls['std'].append(std)\n",
    "                        ls['min'].append(minimum)\n",
    "                        ls['max'].append(maximum)\n",
    "                        ls['image'].append(arr_img)\n",
    "        df = pd.DataFrame(ls)\n",
    "        df['image_r'] = df['path'].map(lambda x: np.asarray(Image.open(x).resize((75, 75))))\n",
    "        return df\n",
    "\n",
    "    def visualize(self):\n",
    "        self.data = self.load_data()\n",
    "        samples, features = self.data.shape\n",
    "\n",
    "        # We are checking if the figures directory exist, if not we are creating it\n",
    "        if os.path.isdir(os.path.join(os.path.join(os.getcwd(),'figures'))) is False:\n",
    "            os.mkdir(os.path.join(os.getcwd(),'figures'))\n",
    "        # Samples per class\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.set(style=\"ticks\", font_scale=1)\n",
    "        ax = sns.countplot(data=self.data, x='class', order=self.data['class'].value_counts().index, palette=\"flare\")\n",
    "        sns.despine(top=True, right=True, left=True, bottom=False)\n",
    "        plt.xticks(rotation=0, fontsize=12)\n",
    "        ax.set_xlabel('Class Type - Diagnosis', fontsize=14, weight='bold')\n",
    "        ax.set(yticklabels=[])\n",
    "        ax.axes.get_yaxis().set_visible(False)\n",
    "        plt.title('Number of Sample X-Ray Images per Class', fontsize=12, weight='bold');\n",
    "\n",
    "        for p in ax.patches:\n",
    "            ax.annotate(\"%.1f%%\" % (100 * float(p.get_height() / samples)),\n",
    "                        (p.get_x() + p.get_width() / 2., abs(p.get_height())),\n",
    "                        ha='center', va='bottom', color='black', xytext=(0, 10), rotation='horizontal',\n",
    "                        textcoords='offset points')\n",
    "        plt.savefig(f'{os.getcwd()}/figures/image_class_distribution.png', dpi=300)\n",
    "\n",
    "        self.plot_sample_images(3, \"sample_images_all_channels\", True)\n",
    "        self.plot_sample_images(3, \"sample_images_single_channels\", False)\n",
    "\n",
    "    #     We are plotting color distribution of images\n",
    "        ax = sns.displot(data=self.data, x='mean', kind=\"kde\", hue='class', fill=False);\n",
    "        plt.title('Images Colour Mean Value Distribution by Class', fontsize=12, weight='bold');\n",
    "        plt.savefig(f'{os.getcwd()}/figures/image_mean_color.png', dpi=300)\n",
    "\n",
    "        #\n",
    "        ax = sns.displot(data=self.data, x='max', kind=\"kde\", hue='class');\n",
    "        plt.title('Images Colour Max Value Distribution by Class', fontsize=12, weight='bold');\n",
    "        plt.savefig(f'{os.getcwd()}/figures/image_max_color.png', dpi=300)\n",
    "\n",
    "        ax = sns.displot(data=self.data, x='min', kind=\"kde\", hue='class');\n",
    "        plt.title('Images Colour Min Value Distribution by Class', fontsize=12, weight='bold');\n",
    "        plt.savefig(f'{os.getcwd()}/figures/image_min_color.png', dpi=300)\n",
    "\n",
    "        # we are plotting the mean and standard deviation of the dataset\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.set(style=\"ticks\", font_scale=1)\n",
    "        ax = sns.scatterplot(data=self.data, x=\"mean\", y=self.data['std'], hue='class', alpha=0.8);\n",
    "        sns.despine(top=True, right=True, left=False, bottom=False)\n",
    "        plt.xticks(rotation=0, fontsize=12)\n",
    "        ax.set_xlabel('Image Channel Colour Mean', fontsize=12, weight='bold')\n",
    "        ax.set_ylabel('Image Channel Colour Standard Deviation', fontsize=12, weight='bold')\n",
    "        plt.title('Mean and Standard Deviation of Image Samples', fontsize=12, weight='bold');\n",
    "        plt.savefig(f'{os.getcwd()}/figures/image_distribution_scatter.png', dpi=300)\n",
    "\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        g = sns.FacetGrid(self.data, col=\"class\", height=5);\n",
    "        g.map_dataframe(sns.scatterplot, x='mean', y='std', hue='class');\n",
    "        g.set_titles(col_template=\"{col_name}\", row_template=\"{row_name}\", size=16)\n",
    "        g.fig.subplots_adjust(top=.7)\n",
    "        # g.fig.suptitle('Mean and Standard Deviation of Image Samples',fontsize=16, weight = 'bold')\n",
    "        axes = g.axes.flatten()\n",
    "        axes[0].set_ylabel('Standard Deviation');\n",
    "        for ax in axes:\n",
    "            ax.set_xlabel('Mean')\n",
    "        g.fig.tight_layout()\n",
    "        plt.savefig(f'{os.getcwd()}/figures/image_distribution_4_plots.png', dpi=300)\n",
    "\n",
    "        #we are plotting samples of the images in an annotation  box\n",
    "\n",
    "        DF_sample = self.data.sample(frac=0.1, replace=False, random_state=1)\n",
    "        paths = DF_sample['path']\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ab = sns.scatterplot(data=DF_sample, x=\"mean\", y='std')\n",
    "        sns.despine(top=True, right=True, left=False, bottom=False)\n",
    "        ax.set_xlabel('Image Channel Colour Mean', fontsize=12, weight='bold')\n",
    "        ax.set_ylabel('Image Channel Colour Standard Deviation', fontsize=14, weight='bold')\n",
    "        plt.title('Mean and Standard Deviation of Image Samples - 10% of Data', fontsize=12, weight='bold');\n",
    "\n",
    "        for x0, y0, path in zip(DF_sample['mean'], DF_sample['std'], paths):\n",
    "            ab = AnnotationBbox(self.getImage(path), (x0, y0), frameon=False)\n",
    "            ax.add_artist(ab)\n",
    "        plt.savefig(f'{os.getcwd()}/figures/image_bounding_box.png', dpi=300)\n",
    "\n",
    "\n",
    "\n",
    "    def getImage(self,path):\n",
    "        imdata = plt.imread(path)\n",
    "        return OffsetImage(imdata, zoom=0.1)\n",
    "\n",
    "    def plot_sample_images(self, n_samples=3, pltname=\"sample_images_all_channels\", channels=True):\n",
    "        fig, m_axs = plt.subplots(4, n_samples, figsize=(4 * n_samples, 3 * 4))\n",
    "        for n_axs, (type_name, type_rows) in zip(m_axs, self.data.sort_values(['class']).groupby('class')):\n",
    "            n_axs[1].set_title(type_name, fontsize=12, weight='bold')\n",
    "            for c_ax, (_, c_row) in zip(n_axs, type_rows.sample(n_samples, random_state=1234).iterrows()):\n",
    "                picture = c_row['path']\n",
    "                if channels:\n",
    "                    image = cv2.imread(picture)\n",
    "                else:\n",
    "                    image = plt.imread(picture)\n",
    "                c_ax.imshow(image)\n",
    "                c_ax.axis('off')\n",
    "\n",
    "        plt.savefig(f'{os.getcwd()}/figures/{pltname}.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "form",
    "id": "Tii5a21TBbJE"
   },
   "outputs": [],
   "source": [
    "#@title We are performing EDA\n",
    "eda =  EDA(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0puAkoKzBpfE"
   },
   "outputs": [],
   "source": [
    "eda.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "form",
    "id": "zuCf5Zm5GA22"
   },
   "outputs": [],
   "source": [
    "#@title We proceeding with data preprocessing step \n",
    "class Preprocessing:\n",
    "    base = None\n",
    "    test_path = None\n",
    "    root = os.getcwd()\n",
    "    basepath = None\n",
    "    dataset = None\n",
    "    paths =  None\n",
    "\n",
    "    def __init__(self, basepath):\n",
    "        self.dataset = 'dataset'\n",
    "        if os.path.isdir(os.path.join(self.root, self.dataset)) is False:\n",
    "            os.mkdir(os.path.join(self.root, self.dataset))\n",
    "        if os.path.isdir(os.path.join(self.root, self.dataset + '/test')) is False:\n",
    "            os.mkdir(os.path.join(self.root, self.dataset + '/test'))\n",
    "        self.dataset = os.path.join(self.root, self.dataset)\n",
    "        self.test_path = os.path.join(self.root, self.dataset + '/test')\n",
    "        self.basepath = basepath\n",
    "        self.paths =  os.listdir(self.basepath)\n",
    "        self.path = os.path.join(self.root,'dataset')\n",
    "\n",
    "    def make_testdir(self):\n",
    "        # We are creating the test directories\n",
    "        for k in range(len(self.paths)):\n",
    "            if os.path.isdir(os.path.join(self.basepath, self.paths[k])):\n",
    "                if os.path.isdir(os.path.join(self.test_path, self.paths[k])) is False:\n",
    "                    os.mkdir(os.path.join(self.test_path, self.paths[k]))\n",
    "\n",
    "    def testset(self,pr=0.1):\n",
    "        # We are creating our test set\n",
    "        if os.path.isdir(self.path) is True:\n",
    "            paths = os.listdir(self.basepath)\n",
    "            pr = pr\n",
    "            for k in range(len(paths)):\n",
    "                if os.path.isdir(os.path.join(self.basepath, self.paths[k])):\n",
    "                    # We are moving 2% of the sample dataset to test\n",
    "\n",
    "                    n = np.int(len(os.listdir(os.path.join(self.basepath, self.paths[k]))) * pr)\n",
    "                    for c in random.sample(glob.glob(os.path.join(self.basepath, self.paths[k] + '/' + self.paths[k] + '*')), n):\n",
    "                        shutil.move(c, os.path.join(self.test_path, self.paths[k]))\n",
    "\n",
    "    def make_testset(self):\n",
    "        self.make_testdir()\n",
    "        self.testset(0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "itIbQ0QSKkKM"
   },
   "outputs": [],
   "source": [
    "pp =  Preprocessing(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "TCN6xx66KuGK"
   },
   "outputs": [],
   "source": [
    "pp.make_testset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellView": "form",
    "id": "lbpSYfuXK0DE"
   },
   "outputs": [],
   "source": [
    "#@title Model training, validation and testing\n",
    "class Modelling:\n",
    "    datapath = None\n",
    "    testpath = None\n",
    "    # augmentation parameters\n",
    "    # you can use preprocessing_function instead of rescale in all generators\n",
    "    # if you are using a pretrained network\n",
    "    train_augmentation_parameters = dict(\n",
    "        rescale=1.0 / 255.0,\n",
    "        rotation_range=45,\n",
    "        width_shift_range=0.15,\n",
    "        height_shift_range=0.2,\n",
    "        fill_mode='nearest',\n",
    "        # crop_and_pad=0.25,\n",
    "        shear_range=16.0,\n",
    "        validation_split=0.2\n",
    "    )\n",
    "    # training parameters\n",
    "    test_augmentation_parameters = dict(\n",
    "        rescale=1.0 / 255.0\n",
    "    )\n",
    "    NUM_CLASSES = 4\n",
    "    BATCH_SIZE = 32\n",
    "    CLASS_MODE = 'categorical'\n",
    "    # CLASS_MODE = 'binary'\n",
    "    COLOR_MODE = 'grayscale'\n",
    "    TARGET_SIZE = (128, 128)\n",
    "    EPOCHS = 100\n",
    "    SEED = 214\n",
    "    train_datagen = None\n",
    "    train_generator = None\n",
    "    test_datagen  =  None\n",
    "    test_generator  =  None\n",
    "    valid_generator = None\n",
    "    model = None\n",
    "    optim_params = None\n",
    "    reduce_lr = None\n",
    "    check_point = None\n",
    "    early_stop = None\n",
    "    y_labels = None\n",
    "    classweight_dict = None\n",
    "    hist = None\n",
    "    y_pred =  None\n",
    "    y_pred_classes  =  None\n",
    "    score  =  None\n",
    "    classweight = None\n",
    "    classes = [\"COVID\", \"Lung_Opacity\", \"Normal\", \"Viral Pneumonia\"]\n",
    "\n",
    "    def __init__(self, datapath, testpath):\n",
    "        self.datapath = datapath\n",
    "        self.testpath = testpath\n",
    "        pass\n",
    "\n",
    "    def augment_data(self):\n",
    "        # Using the training phase generators\n",
    "        self.train_datagen = ImageDataGenerator(**self.train_augmentation_parameters)\n",
    "\n",
    "        self.train_generator = self.train_datagen.flow_from_directory(\n",
    "            self.datapath,  # +\"/train\",\n",
    "            target_size=self.TARGET_SIZE,\n",
    "            batch_size=self.BATCH_SIZE,\n",
    "            class_mode=self.CLASS_MODE,\n",
    "            subset=\"training\"\n",
    "        )\n",
    "\n",
    "        self.valid_generator = self.train_datagen.flow_from_directory(\n",
    "            self.datapath,\n",
    "            target_size=self.TARGET_SIZE,\n",
    "            batch_size=self.BATCH_SIZE,\n",
    "            class_mode=self.CLASS_MODE,\n",
    "            subset=\"validation\"\n",
    "        )\n",
    "\n",
    "        # Test data generator\n",
    "        self.test_datagen = ImageDataGenerator(**self.test_augmentation_parameters)\n",
    "        self.test_generator = self.test_datagen.flow_from_directory(\n",
    "            self.testpath,\n",
    "            target_size=self.TARGET_SIZE,\n",
    "            batch_size=self.BATCH_SIZE,\n",
    "            class_mode=self.CLASS_MODE,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "        print()\n",
    "        print(np.bincount(self.train_generator.classes))\n",
    "        print(np.bincount(self.valid_generator.classes))\n",
    "\n",
    "    def build(self):\n",
    "        K.clear_session()\n",
    "\n",
    "        # base_model = VGG16(weights='imagenet', include_top=False, input_shape=TARGET_SIZE+(3,) ) base_model =\n",
    "        # VGG16(weights='weights/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5', include_top=False,\n",
    "        # input_shape=TARGET_SIZE+(3,))\n",
    "\n",
    "        # base_model = VGG16(weights='imagenet', include_top=False, input_shape=TARGET_SIZE+(3,))\n",
    "\n",
    "        # base_model = VGG19(weights='weights/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5', include_top=False,\n",
    "        # input_shape=TARGET_SIZE+(3,))\n",
    "        base_model = ResNet50(weights='imagenet', include_top=False, input_shape=self.TARGET_SIZE + (3,))\n",
    "        # base_model = DenseNet121( weights='weights/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "        # include_top=False, input_shape=TARGET_SIZE+(3,)) base_model = InceptionV3(\n",
    "        # weights='weights/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5', include_top=False,\n",
    "        # input_shape=TARGET_SIZE+(3,)) can also try other architectures\n",
    "\n",
    "        x = base_model.output\n",
    "        x = Flatten()(x)\n",
    "        # x = Dense(512, activation='relu')(x)\n",
    "        # x = Dropout(0.1)(x)\n",
    "        # x = Dense(256, activation='relu')(x)\n",
    "        # x = Dropout(0.1)(x)\n",
    "        # x = Dense(128, activation='relu')(x)\n",
    "        # x = Dropout(0.1)(x)\n",
    "        # x = Dense(64, activation='relu')(x)\n",
    "        # x = Dropout(0.1)(x)\n",
    "\n",
    "        x = Dense(self.NUM_CLASSES, activation='softmax')(x)\n",
    "\n",
    "        model = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "        # print(model.summary())\n",
    "\n",
    "        # for layer in model.layers[0:-14]:\n",
    "        #     layer.trainable = False\n",
    "\n",
    "        # print_layers(model)\n",
    "        self.model = model\n",
    "\n",
    "    def printmodel(self):\n",
    "        for idx, layer in enumerate(self.model.layers):\n",
    "            print(\"layer {}: {}, trainable: {}\".format(idx, layer.name, layer.trainable))\n",
    "\n",
    "    def get_model(self):\n",
    "        self.build()\n",
    "        self.augment_data()\n",
    "        self.configuremodel()\n",
    "        return self.model\n",
    "\n",
    "    def configuremodel(self):\n",
    "        self.optim_params = dict(\n",
    "            learning_rate=0.0001,\n",
    "            momentum=0.9394867962846013,\n",
    "            decay=0.0001\n",
    "        )\n",
    "\n",
    "        self.model.compile(\n",
    "            loss='categorical_crossentropy',\n",
    "            optimizer=SGD(**self.optim_params),\n",
    "            metrics=['accuracy'])\n",
    "        self.check_point = ModelCheckpoint('weighted_model_v2.best.h5', monitor='val_loss', verbose=1,\n",
    "                                           save_best_only=True, save_weights_only=False, save_freq=1)\n",
    "\n",
    "        self.reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, verbose=1, min_lr=0.00001);\n",
    "\n",
    "        self.early_stop = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=11, verbose=1,\n",
    "                                        restore_best_weights=True);\n",
    "        # y_labels = np.argmax(y_train, axis=1)\n",
    "        self.y_labels = self.train_generator.classes\n",
    "        self.classweight = class_weight.compute_class_weight('balanced', np.unique(self.y_labels), self.y_labels)\n",
    "\n",
    "        self.classweight_dict = {}\n",
    "        for i in range(len(self.classweight)):\n",
    "            self.classweight_dict[i] = self.classweight[i]\n",
    "\n",
    "        print(self.classweight_dict)\n",
    "        print(np.bincount(self.train_generator.classes))\n",
    "\n",
    "    def train(self):\n",
    "        self.hist = self.model.fit(\n",
    "            self.train_generator,\n",
    "            validation_data=self.valid_generator,\n",
    "            epochs=self.EPOCHS\n",
    "        )\n",
    "        train_hist = pd.DataFrame(self.hist.history)\n",
    "        train_hist.to_csv(os.path.join(os.getcwd(), 'figures/train_history.csv'), index=False)\n",
    "        self.plot_acc_loss()\n",
    "        self.model.save(os.path.join(os.getcwd(), \"models/ResNet50.model.h5\"))\n",
    "        self.model.save_weights(os.path.join(os.getcwd(), \"models/ResNet50.weights.model..h5\"))\n",
    "    def plot_acc_loss(self):\n",
    "        # We are checking if the figures directory exist, if not we are creating it\n",
    "        if os.path.isdir(os.path.join(os.path.join(os.getcwd(),'figures'))) is False:\n",
    "            os.mkdir(os.path.join(os.getcwd(),'figures'))\n",
    "\n",
    "        fig = plt.figure(figsize=(8, 6), dpi=150)\n",
    "        plt.plot(self.hist.history['accuracy'])\n",
    "        plt.plot(self.hist.history['val_accuracy'])\n",
    "        plt.title('model accuracy')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'validation'], loc='lower right')\n",
    "        plt.savefig(os.path.join(os.getcwd(), '/figures/train_val_accuracy.png'), dpi=300)\n",
    "\n",
    "        fig = plt.figure(figsize=(8, 6))\n",
    "        plt.plot(self.hist.history['loss'])\n",
    "        plt.plot(self.hist.history['val_loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper right')\n",
    "        plt.savefig(os.path.join(os.getcwd(), '/figures/train_val_loss.png'), dpi=300)\n",
    "    def test(self):\n",
    "        print(np.bincount(self.test_generator.classes), \"\\n\")\n",
    "        self.score = self.model.evaluate(self.test_generator, verbose=0)\n",
    "        # Predic\n",
    "        self.y_pred = self.model.predict_generator(generator=self.test_generator)\n",
    "        # print(y_pred[:10])\n",
    "        # to get the prediction, we pick the class with with the highest probability\n",
    "        self.y_pred_classes = np.argmax(self.y_pred, axis=1)\n",
    "        # y_true = np.argmax(y_val, axis = 1)\n",
    "        y_true = self.test_generator.classes\n",
    "\n",
    "        conf_mtx = confusion_matrix(y_true, self.y_pred_classes)\n",
    "        plot_confusion_matrix(conf_mtx, figsize=(12, 8), hide_ticks=True, cmap=plt.cm.Blues, colorbar=True)\n",
    "        plt.xticks(range(4), self.classes, fontsize=16)\n",
    "        plt.yticks(range(4), self.classes, fontsize=16)\n",
    "        plt.savefig(os.path.join(os.getcwd(), '/figures/test_confusion_matrix.png'), dpi=300)\n",
    "        print('Model Loss: {}, Accuracy: {}'.format(self.score[0], self.score[1]))\n",
    "\n",
    "        # we are analyzing model complexity using ROC\n",
    "        self.plot_roc()\n",
    "    def plot_roc(self):\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "\n",
    "        y_test = to_categorical(self.test_generator.classes)\n",
    "\n",
    "        for i in range(self.NUM_CLASSES):\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_test[:, i], self.y_pred[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "        # for i in range(1):\n",
    "        #     fpr[i], tpr[i], _ = roc_curve(y_true, y_pred)\n",
    "        #     roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        for i in range(self.NUM_CLASSES):\n",
    "            plt.plot(fpr[i], tpr[i], lw=2,\n",
    "                     label='ROC curve of {0} (area = {1:0.3f})'.format(self.classes[i], roc_auc[i]))\n",
    "\n",
    "        # for i in range(1):\n",
    "        #     plt.plot(fpr[i], tpr[i], lw=2,\n",
    "        #              label='ROC curve (area = {:0.3f})'.format(roc_auc[i]))\n",
    "\n",
    "        plt.plot(fpr[0], fpr[0], 'k-', label='random guessing')\n",
    "\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC curve')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(os.getcwd(), 'figures/roc_curve.png'), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "pavVau9KKW2r"
   },
   "outputs": [],
   "source": [
    "test_path =  '/content/dataset/test'\n",
    "datapath  =  base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IrJhXClTLE0i"
   },
   "outputs": [],
   "source": [
    "model = modelling.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KZjBNY70LgYe"
   },
   "outputs": [],
   "source": [
    "modelling.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4NYi__9zLlCq"
   },
   "outputs": [],
   "source": [
    "modelling.test()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "testbed.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
